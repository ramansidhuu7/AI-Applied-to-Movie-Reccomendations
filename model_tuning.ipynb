{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3559860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (10, 1)\n",
      "Shape of y: (10,)\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load cosine similarity matrix (cos_sim)\n",
    "with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "    cos_sim = pickle.load(f)\n",
    "\n",
    "# Load content dataframe (content_df)\n",
    "content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "\n",
    "# Function to predict scores and similarities for similar movies\n",
    "def predict(title, similarity_weight=0.7, top_n=10):\n",
    "    data = content_df.reset_index()\n",
    "    index_movie = data[data['original_title'] == title].index\n",
    "    similarity = cos_sim[index_movie].T\n",
    "    sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "    final_df = pd.concat([data, sim_df], axis=1)\n",
    "    final_df['final_score'] = final_df['score']*(1-similarity_weight) + final_df['similarity']*similarity_weight\n",
    "    final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "    final_df_sorted.set_index('original_title', inplace=True)\n",
    "    return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict('Toy Story', similarity_weight=0.7, top_n=10)\n",
    "\n",
    "# Extract features for LIME\n",
    "X = predictions.drop(columns=['score', 'final_score'])  # Remove 'score' and 'final_score' columns\n",
    "y = pd.Series([0] * len(X))  # Create a dummy target variable (can be any value)\n",
    "\n",
    "# Print the shapes of X and y for verification\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89a390",
   "metadata": {},
   "source": [
    "LIME and SHAP expect X to be a 2-dimensional array where each row represents a sample and each column represents a feature. In this case, X has only one feature column, which may cause issues with LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad483ae4",
   "metadata": {},
   "source": [
    "To resolve this, we need to ensure that X has at least two feature columns, even if one of them is a constant value. Let's modify the code to include an additional dummy feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c78aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated shape of X: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a dummy feature column to X\n",
    "X['dummy_feature'] = np.zeros(len(X))\n",
    "\n",
    "# Ensure that X has the expected shape\n",
    "print(\"Updated shape of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8619aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (10000,), (5000, 2))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[0;32m      6\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(X\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m----> 7\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(X\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply SHAP\u001b[39;00m\n\u001b[0;32m     10\u001b[0m shap_explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(predict, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:355\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    348\u001b[0m     scaled_data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mmean_) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    349\u001b[0m distances \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpairwise_distances(\n\u001b[0;32m    350\u001b[0m         scaled_data,\n\u001b[0;32m    351\u001b[0m         scaled_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    352\u001b[0m         metric\u001b[38;5;241m=\u001b[39mdistance_metric\n\u001b[0;32m    353\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m--> 355\u001b[0m yss \u001b[38;5;241m=\u001b[39m predict_fn(inverse)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# for classification, the model needs to provide a list of tuples - classes\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# along with prediction probabilities\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(title, similarity_weight, top_n)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(title, similarity_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m content_df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m---> 17\u001b[0m     index_movie \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m title]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     18\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m cos_sim[index_movie]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     19\u001b[0m     sim_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(similarity, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:264\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lvalues) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rvalues):\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    269\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    271\u001b[0m ):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Lengths must match to compare', (10000,), (5000, 2))"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns)\n",
    "exp = explainer.explain_instance(X.iloc[0].values, predict, num_features=len(X.columns))\n",
    "\n",
    "# Apply SHAP\n",
    "shap_explainer = shap.Explainer(predict, X)\n",
    "shap_values = shap_explainer(X)\n",
    "\n",
    "# Plot LIME explanation\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f828b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         score  similarity  final_score\n",
      "original_title                                         \n",
      "Toy Story             0.348515    1.000000     0.804554\n",
      "Toy Story 2           0.317785    0.537320     0.471459\n",
      "Toy Story 3           0.336500    0.274778     0.293294\n",
      "Toy Story of Terror!  0.282269    0.294858     0.291081\n",
      "Small Fry             0.256223    0.271027     0.266586\n",
      "Hawaiian Vacation     0.266277    0.263818     0.264556\n",
      "Minions               0.841412    0.005375     0.256186\n",
      "Finding Nemo          0.346184    0.203631     0.246397\n",
      "WALL·E                0.348681    0.196732     0.242317\n",
      "A Bug's Life          0.284637    0.215010     0.235898\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m instance \u001b[38;5;241m=\u001b[39m content_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(instance\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39minstance\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     63\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(instance\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(instance\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:215\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    209\u001b[0m     discretizer \u001b[38;5;241m=\u001b[39m StatsDiscretizer(training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    210\u001b[0m                                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    211\u001b[0m                                    data_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data_stats,\n\u001b[0;32m    212\u001b[0m                                    random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discretizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquartile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer \u001b[38;5;241m=\u001b[39m QuartileDiscretizer(\n\u001b[0;32m    216\u001b[0m             training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    218\u001b[0m             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m discretizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer \u001b[38;5;241m=\u001b[39m DecileDiscretizer(\n\u001b[0;32m    221\u001b[0m             training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    223\u001b[0m             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:178\u001b[0m, in \u001b[0;36mQuartileDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, categorical_features, feature_names, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 178\u001b[0m     BaseDiscretizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, categorical_features,\n\u001b[0;32m    179\u001b[0m                              feature_names, labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    180\u001b[0m                              random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:51\u001b[0m, in \u001b[0;36mBaseDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m check_random_state(random_state)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# To override when implementing a custom binning\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins(data, labels)\n\u001b[0;32m     52\u001b[0m bins \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39munique(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m bins]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Read the stats from data_stats if exists\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:185\u001b[0m, in \u001b[0;36mQuartileDiscretizer.bins\u001b[1;34m(self, data, labels)\u001b[0m\n\u001b[0;32m    183\u001b[0m bins \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_discretize:\n\u001b[1;32m--> 185\u001b[0m     qts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mpercentile(data[:, feature], [\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m75\u001b[39m]))\n\u001b[0;32m    186\u001b[0m     bins\u001b[38;5;241m.\u001b[39mappend(qts)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bins\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4205\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[0;32m   4204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentiles must be in the range [0, 100]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _quantile_unchecked(\n\u001b[0;32m   4206\u001b[0m     a, q, axis, out, overwrite_input, method, keepdims)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4473\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[0;32m   4465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[0;32m   4466\u001b[0m                         q,\n\u001b[0;32m   4467\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4470\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4471\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   4472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ureduce(a,\n\u001b[0;32m   4474\u001b[0m                     func\u001b[38;5;241m=\u001b[39m_quantile_ureduce_func,\n\u001b[0;32m   4475\u001b[0m                     q\u001b[38;5;241m=\u001b[39mq,\n\u001b[0;32m   4476\u001b[0m                     keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m   4477\u001b[0m                     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   4478\u001b[0m                     out\u001b[38;5;241m=\u001b[39mout,\n\u001b[0;32m   4479\u001b[0m                     overwrite_input\u001b[38;5;241m=\u001b[39moverwrite_input,\n\u001b[0;32m   4480\u001b[0m                     method\u001b[38;5;241m=\u001b[39mmethod)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4639\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[0;32m   4637\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4638\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 4639\u001b[0m result \u001b[38;5;241m=\u001b[39m _quantile(arr,\n\u001b[0;32m   4640\u001b[0m                    quantiles\u001b[38;5;241m=\u001b[39mq,\n\u001b[0;32m   4641\u001b[0m                    axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   4642\u001b[0m                    method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   4643\u001b[0m                    out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m   4644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4756\u001b[0m, in \u001b[0;36m_quantile\u001b[1;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[0;32m   4754\u001b[0m     result_shape \u001b[38;5;241m=\u001b[39m virtual_indexes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4755\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mreshape(result_shape)\n\u001b[1;32m-> 4756\u001b[0m     result \u001b[38;5;241m=\u001b[39m _lerp(previous,\n\u001b[0;32m   4757\u001b[0m                    \u001b[38;5;28mnext\u001b[39m,\n\u001b[0;32m   4758\u001b[0m                    gamma,\n\u001b[0;32m   4759\u001b[0m                    out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m   4760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(slices_having_nans):\n\u001b[0;32m   4761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4762\u001b[0m         \u001b[38;5;66;03m# can't write to a scalar\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4573\u001b[0m, in \u001b[0;36m_lerp\u001b[1;34m(a, b, t, out)\u001b[0m\n\u001b[0;32m   4559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lerp\u001b[39m(a, b, t, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4561\u001b[0m \u001b[38;5;124;03m    Compute the linear interpolation weighted by gamma on each point of\u001b[39;00m\n\u001b[0;32m   4562\u001b[0m \u001b[38;5;124;03m    two same shape array.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;124;03m        Output array.\u001b[39;00m\n\u001b[0;32m   4572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4573\u001b[0m     diff_b_a \u001b[38;5;241m=\u001b[39m subtract(b, a)\n\u001b[0;32m   4574\u001b[0m     \u001b[38;5;66;03m# asanyarray is a stop-gap until gh-13105\u001b[39;00m\n\u001b[0;32m   4575\u001b[0m     lerp_interpolation \u001b[38;5;241m=\u001b[39m asanyarray(add(a, diff_b_a \u001b[38;5;241m*\u001b[39m t, out\u001b[38;5;241m=\u001b[39mout))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "import shap\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the content dataframe\n",
    "content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "\n",
    "# Load the cosine similarity matrix\n",
    "with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "    cos_sim = pickle.load(f)\n",
    "\n",
    "# Define the predict function\n",
    "def predict(title, similarity_weight=0.7, top_n=10):\n",
    "    \"\"\"\n",
    "    Predicts top movies similar to the given movie title based on similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): The title of the movie for which predictions are to be made.\n",
    "    - similarity_weight (float): Weightage given to similarity score in the final scoring. Default is 0.7.\n",
    "    - top_n (int): Number of top similar movies to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing top similar movies along with their scores and similarities.\n",
    "    \"\"\"\n",
    "    # Reset index of content_df DataFrame\n",
    "    data = content_df.reset_index()\n",
    "    \n",
    "    # Get the index of the movie with the given title\n",
    "    index_movie = data[data['original_title'] == title].index\n",
    "    \n",
    "    # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "    similarity = cos_sim[index_movie].T\n",
    "    \n",
    "    # Create a DataFrame containing similarity scores\n",
    "    sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "    \n",
    "    # Concatenate the similarity DataFrame with the data DataFrame\n",
    "    final_df = pd.concat([data, sim_df], axis=1)\n",
    "    \n",
    "    # Calculate final score using similarity_weight\n",
    "    final_df['final_score'] = final_df['score']*(1-similarity_weight) + final_df['similarity']*similarity_weight\n",
    "    \n",
    "    # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "    final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "    \n",
    "    # Set 'original_title' as index\n",
    "    final_df_sorted.set_index('original_title', inplace=True)\n",
    "    \n",
    "    # Return DataFrame containing scores and similarities of top similar movies\n",
    "    return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "\n",
    "# Example usage of predict function\n",
    "result = predict('Toy Story', similarity_weight=0.7, top_n=10)\n",
    "print(result)\n",
    "\n",
    "# Create a single instance DataFrame for explanation\n",
    "instance = content_df.head(1)\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime_tabular.LimeTabularExplainer(instance.values, feature_names=instance.columns)\n",
    "exp = explainer.explain_instance(instance.iloc[0].values, predict, num_features=len(instance.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e68a4",
   "metadata": {},
   "source": [
    "There's a problem with the data types in the instance DataFrame, particularly when computing quartiles for discretization. The error message \"TypeError: unsupported operand type(s) for -: 'str' and 'str'\" suggests that the columns in the instance DataFrame contain string values instead of numerical values.\n",
    "\n",
    "To address this issue, we need to ensure that the data in the instance DataFrame is in a numerical format before passing it to the LIME explainer. We can achieve this by selecting only the numerical columns from the content_df DataFrame when creating the instance DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3c234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         score  similarity  final_score\n",
      "original_title                                         \n",
      "Toy Story             0.348515    1.000000     0.804554\n",
      "Toy Story 2           0.317785    0.537320     0.471459\n",
      "Toy Story 3           0.336500    0.274778     0.293294\n",
      "Toy Story of Terror!  0.282269    0.294858     0.291081\n",
      "Small Fry             0.256223    0.271027     0.266586\n",
      "Hawaiian Vacation     0.266277    0.263818     0.264556\n",
      "Minions               0.841412    0.005375     0.256186\n",
      "Finding Nemo          0.346184    0.203631     0.246397\n",
      "WALL·E                0.348681    0.196732     0.242317\n",
      "A Bug's Life          0.284637    0.215010     0.235898\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (10000,), (5000, 3))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[0;32m     61\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(instance\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39minstance\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 62\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(instance\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(instance\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:355\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    348\u001b[0m     scaled_data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mmean_) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    349\u001b[0m distances \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpairwise_distances(\n\u001b[0;32m    350\u001b[0m         scaled_data,\n\u001b[0;32m    351\u001b[0m         scaled_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    352\u001b[0m         metric\u001b[38;5;241m=\u001b[39mdistance_metric\n\u001b[0;32m    353\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m--> 355\u001b[0m yss \u001b[38;5;241m=\u001b[39m predict_fn(inverse)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# for classification, the model needs to provide a list of tuples - classes\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# along with prediction probabilities\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[13], line 30\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(title, similarity_weight, top_n)\u001b[0m\n\u001b[0;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m content_df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the index of the movie with the given title\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m index_movie \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m title]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Transpose cosine similarity matrix to get similarities for the given movie\u001b[39;00m\n\u001b[0;32m     33\u001b[0m similarity \u001b[38;5;241m=\u001b[39m cos_sim[index_movie]\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:264\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lvalues) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rvalues):\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    269\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    271\u001b[0m ):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Lengths must match to compare', (10000,), (5000, 3))"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the content dataframe\n",
    "content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "\n",
    "# Load the cosine similarity matrix\n",
    "with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "    cos_sim = pickle.load(f)\n",
    "\n",
    "# Define the predict function\n",
    "def predict(title, similarity_weight=0.7, top_n=10):\n",
    "    \"\"\"\n",
    "    Predicts top movies similar to the given movie title based on similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): The title of the movie for which predictions are to be made.\n",
    "    - similarity_weight (float): Weightage given to similarity score in the final scoring. Default is 0.7.\n",
    "    - top_n (int): Number of top similar movies to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing top similar movies along with their scores and similarities.\n",
    "    \"\"\"\n",
    "    # Reset index of content_df DataFrame\n",
    "    data = content_df.reset_index()\n",
    "    \n",
    "    # Get the index of the movie with the given title\n",
    "    index_movie = data[data['original_title'] == title].index\n",
    "    \n",
    "    # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "    similarity = cos_sim[index_movie].T\n",
    "    \n",
    "    # Create a DataFrame containing similarity scores\n",
    "    sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "    \n",
    "    # Concatenate the similarity DataFrame with the data DataFrame\n",
    "    final_df = pd.concat([data, sim_df], axis=1)\n",
    "    \n",
    "    # Calculate final score using similarity_weight\n",
    "    final_df['final_score'] = final_df['score']*(1-similarity_weight) + final_df['similarity']*similarity_weight\n",
    "    \n",
    "    # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "    final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "    \n",
    "    # Set 'original_title' as index\n",
    "    final_df_sorted.set_index('original_title', inplace=True)\n",
    "    \n",
    "    # Return DataFrame containing scores and similarities of top similar movies\n",
    "    return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "\n",
    "# Example usage of predict function\n",
    "result = predict('Toy Story', similarity_weight=0.7, top_n=10)\n",
    "print(result)\n",
    "\n",
    "# Create a single instance DataFrame for explanation\n",
    "instance = content_df.select_dtypes(include='number').head(1)  # Select only numerical columns\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime_tabular.LimeTabularExplainer(instance.values, feature_names=instance.columns)\n",
    "exp = explainer.explain_instance(instance.iloc[0].values, predict, num_features=len(instance.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed007d7",
   "metadata": {},
   "source": [
    "The error indicates that there is an inconsistency in the lengths of the data used for comparison in the predict function. Specifically, the length of the data obtained from content_df does not match the length of the data obtained from the input title. This mismatch leads to the ValueError.\n",
    "\n",
    "To resolve this issue, ensure that the data used for comparison has the same length. Here's the part of the code that needs to be adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb77f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of movie: Int64Index([131], dtype='int64')\n",
      "Similarity: [[0.00537545]\n",
      " [0.00694583]\n",
      " [0.00015944]\n",
      " ...\n",
      " [0.02084702]\n",
      " [0.00368631]\n",
      " [0.00239486]]\n",
      "                         score  similarity  final_score\n",
      "original_title                                         \n",
      "Toy Story             0.348515    1.000000     0.804554\n",
      "Toy Story 2           0.317785    0.537320     0.471459\n",
      "Toy Story 3           0.336500    0.274778     0.293294\n",
      "Toy Story of Terror!  0.282269    0.294858     0.291081\n",
      "Small Fry             0.256223    0.271027     0.266586\n",
      "Hawaiian Vacation     0.266277    0.263818     0.264556\n",
      "Minions               0.841412    0.005375     0.256186\n",
      "Finding Nemo          0.346184    0.203631     0.246397\n",
      "WALL·E                0.348681    0.196732     0.242317\n",
      "A Bug's Life          0.284637    0.215010     0.235898\n"
     ]
    }
   ],
   "source": [
    "def predict(title, similarity_weight=0.7, top_n=10):\n",
    "    try:\n",
    "        # Load the content dataframe\n",
    "        content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "        \n",
    "        # Load the cosine similarity matrix\n",
    "        with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "            cos_sim = pickle.load(f)\n",
    "        \n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        print(\"Index of movie:\", index_movie)  # Print index of the movie\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        print(\"Similarity:\", similarity)  # Print similarity scores\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Return DataFrame containing scores and similarities of top similar movies\n",
    "        return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "result = predict('Toy Story', similarity_weight=0.7, top_n=10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "changing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b2bd4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of movie: Int64Index([1], dtype='int64')\n",
      "Similarity: [[1.23688295e-02]\n",
      " [1.00000000e+00]\n",
      " [5.73268467e-04]\n",
      " ...\n",
      " [2.95992028e-03]\n",
      " [1.58281315e-03]\n",
      " [1.02829603e-03]]\n",
      "                                                 score  similarity  \\\n",
      "original_title                                                       \n",
      "Big Hero 6                                    0.565385    1.000000   \n",
      "Despicable Me 3                               0.268309    0.882704   \n",
      "Treasure Planet                               0.306237    0.751305   \n",
      "Oliver & Company                              0.253648    0.660418   \n",
      "Toy Story 3                                   0.336500    0.654969   \n",
      "That's What I Am                              0.238026    0.617376   \n",
      "The Madagascar Penguins in a Christmas Caper  0.226295    0.616734   \n",
      "The Curse of the Were-Rabbit                  0.275298    0.296992   \n",
      "Buried                                        0.259472    0.279776   \n",
      "Ice Age: Dawn of the Dinosaurs                0.261427    0.252847   \n",
      "猫の恩返し                                         0.293989    0.250492   \n",
      "Le Tableau                                    0.239715    0.249453   \n",
      "Wreck-It Ralph                                0.301033    0.063103   \n",
      "Paperman                                      0.343093    0.052453   \n",
      "Tangled                                       0.320802    0.051045   \n",
      "Bolt                                          0.251409    0.050614   \n",
      "Zootopia                                      0.352872    0.043270   \n",
      "Mr. Peabody & Sherman                         0.267892    0.039423   \n",
      "Monsters, Inc.                                0.340781    0.037180   \n",
      "Winnie the Pooh                               0.263771    0.035781   \n",
      "\n",
      "                                              final_score  \n",
      "original_title                                             \n",
      "Big Hero 6                                       1.000000  \n",
      "Despicable Me 3                                  0.882704  \n",
      "Treasure Planet                                  0.751305  \n",
      "Oliver & Company                                 0.660418  \n",
      "Toy Story 3                                      0.654969  \n",
      "That's What I Am                                 0.617376  \n",
      "The Madagascar Penguins in a Christmas Caper     0.616734  \n",
      "The Curse of the Were-Rabbit                     0.296992  \n",
      "Buried                                           0.279776  \n",
      "Ice Age: Dawn of the Dinosaurs                   0.252847  \n",
      "猫の恩返し                                            0.250492  \n",
      "Le Tableau                                       0.249453  \n",
      "Wreck-It Ralph                                   0.063103  \n",
      "Paperman                                         0.052453  \n",
      "Tangled                                          0.051045  \n",
      "Bolt                                             0.050614  \n",
      "Zootopia                                         0.043270  \n",
      "Mr. Peabody & Sherman                            0.039423  \n",
      "Monsters, Inc.                                   0.037180  \n",
      "Winnie the Pooh                                  0.035781  \n"
     ]
    }
   ],
   "source": [
    "def predict(title, similarity_weight=1, top_n=20):\n",
    "    try:\n",
    "        # Load the content dataframe\n",
    "        content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "        \n",
    "        # Load the cosine similarity matrix\n",
    "        with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "            cos_sim = pickle.load(f)\n",
    "        \n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        print(\"Index of movie:\", index_movie)  # Print index of the movie\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        print(\"Similarity:\", similarity)  # Print similarity scores\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Return DataFrame containing scores and similarities of top similar movies\n",
    "        return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "result = predict('Big Hero 6', similarity_weight=1, top_n=20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663729f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              index  popularity  \\\n",
      "original_title                                                    \n",
      "Big Hero 6                                        1    0.390602   \n",
      "Despicable Me 3                                2227    0.066908   \n",
      "Treasure Planet                                 691    0.029484   \n",
      "Oliver & Company                               3294    0.019751   \n",
      "Toy Story 3                                     223    0.030990   \n",
      "That's What I Am                               4944    0.014320   \n",
      "The Madagascar Penguins in a Christmas Caper   6922    0.005711   \n",
      "The Curse of the Were-Rabbit                   1825    0.021491   \n",
      "Buried                                         2778    0.013497   \n",
      "Ice Age: Dawn of the Dinosaurs                 2634    0.023709   \n",
      "猫の恩返し                                          1076    0.021259   \n",
      "Le Tableau                                     4746    0.004478   \n",
      "Wreck-It Ralph                                  843    0.025019   \n",
      "Paperman                                        170    0.013148   \n",
      "Tangled                                         407    0.026822   \n",
      "Bolt                                           3507    0.028459   \n",
      "Zootopia                                        107    0.047535   \n",
      "Mr. Peabody & Sherman                          2254    0.017341   \n",
      "Monsters, Inc.                                  187    0.048257   \n",
      "Winnie the Pooh                                2487    0.013439   \n",
      "\n",
      "                                              weighted_average     score  \\\n",
      "original_title                                                             \n",
      "Big Hero 6                                            0.827560  0.565385   \n",
      "Despicable Me 3                                       0.570410  0.268309   \n",
      "Treasure Planet                                       0.721368  0.306237   \n",
      "Oliver & Company                                      0.604494  0.253648   \n",
      "Toy Story 3                                           0.794765  0.336500   \n",
      "That's What I Am                                      0.573585  0.238026   \n",
      "The Madagascar Penguins in a Christmas Caper          0.557172  0.226295   \n",
      "The Curse of the Were-Rabbit                          0.656009  0.275298   \n",
      "Buried                                                0.628435  0.259472   \n",
      "Ice Age: Dawn of the Dinosaurs                        0.618003  0.261427   \n",
      "猫の恩返し                                                 0.703084  0.293989   \n",
      "Le Tableau                                            0.592571  0.239715   \n",
      "Wreck-It Ralph                                        0.715053  0.301033   \n",
      "Paperman                                              0.838009  0.343093   \n",
      "Tangled                                               0.761771  0.320802   \n",
      "Bolt                                                  0.585833  0.251409   \n",
      "Zootopia                                              0.810878  0.352872   \n",
      "Mr. Peabody & Sherman                                 0.643719  0.267892   \n",
      "Monsters, Inc.                                        0.779568  0.340781   \n",
      "Winnie the Pooh                                       0.639270  0.263771   \n",
      "\n",
      "                                                                                   bag_of_words  \\\n",
      "original_title                                                                                    \n",
      "Big Hero 6                                    false adventure family animation action comedy...   \n",
      "Despicable Me 3                               false action animation adventure family comedy...   \n",
      "Treasure Planet                               false adventure animation family fantasy scien...   \n",
      "Oliver & Company                              false animation comedy family this animated ta...   \n",
      "Toy Story 3                                   false animation family comedy woody buzz and t...   \n",
      "That's What I Am                              false comedy drama family a comingofage story ...   \n",
      "The Madagascar Penguins in a Christmas Caper  false animation comedy family during the holid...   \n",
      "The Curse of the Were-Rabbit                  false adventure animation comedy family cheese...   \n",
      "Buried                                        false drama thriller mystery paul is a us truc...   \n",
      "Ice Age: Dawn of the Dinosaurs                false animation comedy family adventure times ...   \n",
      "猫の恩返し                                         false adventure fantasy animation drama family...   \n",
      "Le Tableau                                    false fantasy animation three characters livin...   \n",
      "Wreck-It Ralph                                false family animation comedy adventure wrecki...   \n",
      "Paperman                                      false animation family romance an urban office...   \n",
      "Tangled                                       false animation family when the kingdoms most ...   \n",
      "Bolt                                          false animation family adventure comedy bolt i...   \n",
      "Zootopia                                      false animation adventure family comedy determ...   \n",
      "Mr. Peabody & Sherman                         false animation adventure family a young boy a...   \n",
      "Monsters, Inc.                                false animation comedy family james sullivan a...   \n",
      "Winnie the Pooh                               false animation family during an ordinary day ...   \n",
      "\n",
      "                                              similarity  final_score  \n",
      "original_title                                                         \n",
      "Big Hero 6                                      1.000000     1.000000  \n",
      "Despicable Me 3                                 0.882704     0.882704  \n",
      "Treasure Planet                                 0.751305     0.751305  \n",
      "Oliver & Company                                0.660418     0.660418  \n",
      "Toy Story 3                                     0.654969     0.654969  \n",
      "That's What I Am                                0.617376     0.617376  \n",
      "The Madagascar Penguins in a Christmas Caper    0.616734     0.616734  \n",
      "The Curse of the Were-Rabbit                    0.296992     0.296992  \n",
      "Buried                                          0.279776     0.279776  \n",
      "Ice Age: Dawn of the Dinosaurs                  0.252847     0.252847  \n",
      "猫の恩返し                                           0.250492     0.250492  \n",
      "Le Tableau                                      0.249453     0.249453  \n",
      "Wreck-It Ralph                                  0.063103     0.063103  \n",
      "Paperman                                        0.052453     0.052453  \n",
      "Tangled                                         0.051045     0.051045  \n",
      "Bolt                                            0.050614     0.050614  \n",
      "Zootopia                                        0.043270     0.043270  \n",
      "Mr. Peabody & Sherman                           0.039423     0.039423  \n",
      "Monsters, Inc.                                  0.037180     0.037180  \n",
      "Winnie the Pooh                                 0.035781     0.035781  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(features\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     55\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(features\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(features\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Apply SHAP\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:215\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    209\u001b[0m     discretizer \u001b[38;5;241m=\u001b[39m StatsDiscretizer(training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    210\u001b[0m                                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    211\u001b[0m                                    data_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data_stats,\n\u001b[0;32m    212\u001b[0m                                    random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discretizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquartile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer \u001b[38;5;241m=\u001b[39m QuartileDiscretizer(\n\u001b[0;32m    216\u001b[0m             training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    218\u001b[0m             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m discretizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer \u001b[38;5;241m=\u001b[39m DecileDiscretizer(\n\u001b[0;32m    221\u001b[0m             training_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features,\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, labels\u001b[38;5;241m=\u001b[39mtraining_labels,\n\u001b[0;32m    223\u001b[0m             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:178\u001b[0m, in \u001b[0;36mQuartileDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, categorical_features, feature_names, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 178\u001b[0m     BaseDiscretizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, categorical_features,\n\u001b[0;32m    179\u001b[0m                              feature_names, labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    180\u001b[0m                              random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:51\u001b[0m, in \u001b[0;36mBaseDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m check_random_state(random_state)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# To override when implementing a custom binning\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins(data, labels)\n\u001b[0;32m     52\u001b[0m bins \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39munique(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m bins]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Read the stats from data_stats if exists\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\discretize.py:185\u001b[0m, in \u001b[0;36mQuartileDiscretizer.bins\u001b[1;34m(self, data, labels)\u001b[0m\n\u001b[0;32m    183\u001b[0m bins \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_discretize:\n\u001b[1;32m--> 185\u001b[0m     qts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mpercentile(data[:, feature], [\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m75\u001b[39m]))\n\u001b[0;32m    186\u001b[0m     bins\u001b[38;5;241m.\u001b[39mappend(qts)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bins\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4205\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[0;32m   4204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentiles must be in the range [0, 100]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _quantile_unchecked(\n\u001b[0;32m   4206\u001b[0m     a, q, axis, out, overwrite_input, method, keepdims)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4473\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[0;32m   4465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[0;32m   4466\u001b[0m                         q,\n\u001b[0;32m   4467\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4470\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4471\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   4472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ureduce(a,\n\u001b[0;32m   4474\u001b[0m                     func\u001b[38;5;241m=\u001b[39m_quantile_ureduce_func,\n\u001b[0;32m   4475\u001b[0m                     q\u001b[38;5;241m=\u001b[39mq,\n\u001b[0;32m   4476\u001b[0m                     keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m   4477\u001b[0m                     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   4478\u001b[0m                     out\u001b[38;5;241m=\u001b[39mout,\n\u001b[0;32m   4479\u001b[0m                     overwrite_input\u001b[38;5;241m=\u001b[39moverwrite_input,\n\u001b[0;32m   4480\u001b[0m                     method\u001b[38;5;241m=\u001b[39mmethod)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4639\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[0;32m   4637\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4638\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 4639\u001b[0m result \u001b[38;5;241m=\u001b[39m _quantile(arr,\n\u001b[0;32m   4640\u001b[0m                    quantiles\u001b[38;5;241m=\u001b[39mq,\n\u001b[0;32m   4641\u001b[0m                    axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   4642\u001b[0m                    method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   4643\u001b[0m                    out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m   4644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4756\u001b[0m, in \u001b[0;36m_quantile\u001b[1;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[0;32m   4754\u001b[0m     result_shape \u001b[38;5;241m=\u001b[39m virtual_indexes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4755\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mreshape(result_shape)\n\u001b[1;32m-> 4756\u001b[0m     result \u001b[38;5;241m=\u001b[39m _lerp(previous,\n\u001b[0;32m   4757\u001b[0m                    \u001b[38;5;28mnext\u001b[39m,\n\u001b[0;32m   4758\u001b[0m                    gamma,\n\u001b[0;32m   4759\u001b[0m                    out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m   4760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(slices_having_nans):\n\u001b[0;32m   4761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4762\u001b[0m         \u001b[38;5;66;03m# can't write to a scalar\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4573\u001b[0m, in \u001b[0;36m_lerp\u001b[1;34m(a, b, t, out)\u001b[0m\n\u001b[0;32m   4559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lerp\u001b[39m(a, b, t, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4561\u001b[0m \u001b[38;5;124;03m    Compute the linear interpolation weighted by gamma on each point of\u001b[39;00m\n\u001b[0;32m   4562\u001b[0m \u001b[38;5;124;03m    two same shape array.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;124;03m        Output array.\u001b[39;00m\n\u001b[0;32m   4572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4573\u001b[0m     diff_b_a \u001b[38;5;241m=\u001b[39m subtract(b, a)\n\u001b[0;32m   4574\u001b[0m     \u001b[38;5;66;03m# asanyarray is a stop-gap until gh-13105\u001b[39;00m\n\u001b[0;32m   4575\u001b[0m     lerp_interpolation \u001b[38;5;241m=\u001b[39m asanyarray(add(a, diff_b_a \u001b[38;5;241m*\u001b[39m t, out\u001b[38;5;241m=\u001b[39mout))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "# Load the content dataframe\n",
    "content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "\n",
    "# Load the cosine similarity matrix\n",
    "with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "    cos_sim = pickle.load(f)\n",
    "\n",
    "# Function to predict top similar movies\n",
    "def predict(title, similarity_weight=1, top_n=20):\n",
    "    try:\n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Get the features for LIME\n",
    "        features = final_df_sorted.drop(columns=['score', 'similarity', 'final_score'])\n",
    "        \n",
    "        return final_df_sorted, features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "result, features = predict('Big Hero 6', similarity_weight=1, top_n=20)\n",
    "print(result)\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(features.values, feature_names=features.columns)\n",
    "exp = explainer.explain_instance(features.iloc[0].values, predict, num_features=len(features.columns))\n",
    "\n",
    "# Apply SHAP\n",
    "shap_explainer = shap.Explainer(predict, features)\n",
    "shap_values = shap_explainer.shap_values(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a4edcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: ('Lengths must match to compare', (10000,), (5000, 23))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[0;32m      5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(features_encoded\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39mfeatures_encoded\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m----> 6\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(features_encoded\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(features_encoded\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:360\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# for classification, the model needs to provide a list of tuples - classes\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# along with prediction probabilities\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIME does not currently support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier models without probability \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores. If this conflicts with your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse case, please let us know: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/datascienceinc/lime/issues/16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical variables\n",
    "features_encoded = pd.get_dummies(features)\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(features_encoded.values, feature_names=features_encoded.columns)\n",
    "exp = explainer.explain_instance(features_encoded.iloc[0].values, predict, num_features=len(features_encoded.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1e7dd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 score  similarity  \\\n",
      "original_title                                                       \n",
      "Big Hero 6                                    0.565385    1.000000   \n",
      "Despicable Me 3                               0.268309    0.882704   \n",
      "Treasure Planet                               0.306237    0.751305   \n",
      "Oliver & Company                              0.253648    0.660418   \n",
      "Toy Story 3                                   0.336500    0.654969   \n",
      "That's What I Am                              0.238026    0.617376   \n",
      "The Madagascar Penguins in a Christmas Caper  0.226295    0.616734   \n",
      "The Curse of the Were-Rabbit                  0.275298    0.296992   \n",
      "Buried                                        0.259472    0.279776   \n",
      "Ice Age: Dawn of the Dinosaurs                0.261427    0.252847   \n",
      "猫の恩返し                                         0.293989    0.250492   \n",
      "Le Tableau                                    0.239715    0.249453   \n",
      "Wreck-It Ralph                                0.301033    0.063103   \n",
      "Paperman                                      0.343093    0.052453   \n",
      "Tangled                                       0.320802    0.051045   \n",
      "Bolt                                          0.251409    0.050614   \n",
      "Zootopia                                      0.352872    0.043270   \n",
      "Mr. Peabody & Sherman                         0.267892    0.039423   \n",
      "Monsters, Inc.                                0.340781    0.037180   \n",
      "Winnie the Pooh                               0.263771    0.035781   \n",
      "\n",
      "                                              final_score  \n",
      "original_title                                             \n",
      "Big Hero 6                                       1.000000  \n",
      "Despicable Me 3                                  0.882704  \n",
      "Treasure Planet                                  0.751305  \n",
      "Oliver & Company                                 0.660418  \n",
      "Toy Story 3                                      0.654969  \n",
      "That's What I Am                                 0.617376  \n",
      "The Madagascar Penguins in a Christmas Caper     0.616734  \n",
      "The Curse of the Were-Rabbit                     0.296992  \n",
      "Buried                                           0.279776  \n",
      "Ice Age: Dawn of the Dinosaurs                   0.252847  \n",
      "猫の恩返し                                            0.250492  \n",
      "Le Tableau                                       0.249453  \n",
      "Wreck-It Ralph                                   0.063103  \n",
      "Paperman                                         0.052453  \n",
      "Tangled                                          0.051045  \n",
      "Bolt                                             0.050614  \n",
      "Zootopia                                         0.043270  \n",
      "Mr. Peabody & Sherman                            0.039423  \n",
      "Monsters, Inc.                                   0.037180  \n",
      "Winnie the Pooh                                  0.035781  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m features_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Apply LIME\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(features_encoded\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39mfeatures_encoded\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     56\u001b[0m exp \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(features_encoded\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, predict, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(features_encoded\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lime\\lime_tabular.py:258\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Though set has no role to play if training data stats are provided\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mStandardScaler(with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit(training_data)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_values \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_frequencies \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    973\u001b[0m         )\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    976\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Define the predict function\n",
    "def predict(title, similarity_weight=1, top_n=20):\n",
    "    try:\n",
    "        # Load the content dataframe\n",
    "        content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "        \n",
    "        # Load the cosine similarity matrix\n",
    "        with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "            cos_sim = pickle.load(f)\n",
    "        \n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Return DataFrame containing scores and similarities of top similar movies\n",
    "        return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "result = predict('Big Hero 6', similarity_weight=1, top_n=20)\n",
    "print(result)\n",
    "\n",
    "# Load your features_encoded DataFrame\n",
    "features_encoded = pd.DataFrame()\n",
    "\n",
    "# Apply LIME\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(features_encoded.values, feature_names=features_encoded.columns)\n",
    "exp = explainer.explain_instance(features_encoded.iloc[0].values, predict, num_features=len(features_encoded.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86c5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 score  similarity  \\\n",
      "original_title                                                       \n",
      "Big Hero 6                                    0.565385    1.000000   \n",
      "Despicable Me 3                               0.268309    0.882704   \n",
      "Treasure Planet                               0.306237    0.751305   \n",
      "Oliver & Company                              0.253648    0.660418   \n",
      "Toy Story 3                                   0.336500    0.654969   \n",
      "That's What I Am                              0.238026    0.617376   \n",
      "The Madagascar Penguins in a Christmas Caper  0.226295    0.616734   \n",
      "The Curse of the Were-Rabbit                  0.275298    0.296992   \n",
      "Buried                                        0.259472    0.279776   \n",
      "Ice Age: Dawn of the Dinosaurs                0.261427    0.252847   \n",
      "猫の恩返し                                         0.293989    0.250492   \n",
      "Le Tableau                                    0.239715    0.249453   \n",
      "Wreck-It Ralph                                0.301033    0.063103   \n",
      "Paperman                                      0.343093    0.052453   \n",
      "Tangled                                       0.320802    0.051045   \n",
      "Bolt                                          0.251409    0.050614   \n",
      "Zootopia                                      0.352872    0.043270   \n",
      "Mr. Peabody & Sherman                         0.267892    0.039423   \n",
      "Monsters, Inc.                                0.340781    0.037180   \n",
      "Winnie the Pooh                               0.263771    0.035781   \n",
      "\n",
      "                                              final_score  \n",
      "original_title                                             \n",
      "Big Hero 6                                       1.000000  \n",
      "Despicable Me 3                                  0.882704  \n",
      "Treasure Planet                                  0.751305  \n",
      "Oliver & Company                                 0.660418  \n",
      "Toy Story 3                                      0.654969  \n",
      "That's What I Am                                 0.617376  \n",
      "The Madagascar Penguins in a Christmas Caper     0.616734  \n",
      "The Curse of the Were-Rabbit                     0.296992  \n",
      "Buried                                           0.279776  \n",
      "Ice Age: Dawn of the Dinosaurs                   0.252847  \n",
      "猫の恩返し                                            0.250492  \n",
      "Le Tableau                                       0.249453  \n",
      "Wreck-It Ralph                                   0.063103  \n",
      "Paperman                                         0.052453  \n",
      "Tangled                                          0.051045  \n",
      "Bolt                                             0.050614  \n",
      "Zootopia                                         0.043270  \n",
      "Mr. Peabody & Sherman                            0.039423  \n",
      "Monsters, Inc.                                   0.037180  \n",
      "Winnie the Pooh                                  0.035781  \n",
      "The features_encoded DataFrame is empty. Please populate it with data before applying LIME.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Define the predict function\n",
    "def predict(title, similarity_weight=1, top_n=20):\n",
    "    try:\n",
    "        # Load the content dataframe\n",
    "        content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "        \n",
    "        # Load the cosine similarity matrix\n",
    "        with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "            cos_sim = pickle.load(f)\n",
    "        \n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Return DataFrame containing scores and similarities of top similar movies\n",
    "        return final_df_sorted[['score', 'similarity', 'final_score']]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "result = predict('Big Hero 6', similarity_weight=1, top_n=20)\n",
    "print(result)\n",
    "\n",
    "# Load your features_encoded DataFrame\n",
    "features_encoded = pd.DataFrame()  # Make sure this DataFrame is not empty before applying LIME\n",
    "\n",
    "# Check if features_encoded DataFrame is not empty\n",
    "if not features_encoded.empty:\n",
    "    # Apply LIME\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(features_encoded.values, feature_names=features_encoded.columns)\n",
    "    exp = explainer.explain_instance(features_encoded.iloc[0].values, predict, num_features=len(features_encoded.columns))\n",
    "else:\n",
    "    print(\"The features_encoded DataFrame is empty. Please populate it with data before applying LIME.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fa573f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'ExactExplainer' object has no attribute 'shap_values'\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Define the function to get SHAP explanation\n",
    "def explain_recommendation_with_shap(title, similarity_weight=1, top_n=20):\n",
    "    try:\n",
    "        # Load the content dataframe\n",
    "        content_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\content_df.csv\")\n",
    "        \n",
    "        # Load the cosine similarity matrix\n",
    "        with open(r\"C:\\Users\\ASUS\\Desktop\\documents canada\\cosine_similarity1.pkl\", 'rb') as f:\n",
    "            cos_sim = pickle.load(f)\n",
    "        \n",
    "        # Reset index of content_df DataFrame\n",
    "        data = content_df.reset_index()\n",
    "        \n",
    "        # Get the index of the movie with the given title\n",
    "        index_movie = data[data['original_title'] == title].index\n",
    "        \n",
    "        # Transpose cosine similarity matrix to get similarities for the given movie\n",
    "        similarity = cos_sim[index_movie].T\n",
    "        \n",
    "        # Create a DataFrame containing similarity scores\n",
    "        sim_df = pd.DataFrame(similarity, columns=['similarity'])\n",
    "        \n",
    "        # Concatenate the similarity DataFrame with the data DataFrame\n",
    "        final_df = pd.concat([data, sim_df], axis=1)\n",
    "        \n",
    "        # Calculate final score using similarity_weight\n",
    "        final_df['final_score'] = final_df['score'] * (1 - similarity_weight) + final_df['similarity'] * similarity_weight\n",
    "        \n",
    "        # Sort DataFrame based on final score in descending order and select top_n movies\n",
    "        final_df_sorted = final_df.sort_values(by='final_score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Set 'original_title' as index\n",
    "        final_df_sorted.set_index('original_title', inplace=True)\n",
    "        \n",
    "        # Get the top recommendation\n",
    "        top_recommendation = final_df_sorted.iloc[0]\n",
    "        \n",
    "        # Get feature matrix for SHAP\n",
    "        X = final_df_sorted.drop(['score', 'similarity', 'final_score'], axis=1)\n",
    "        \n",
    "        # Get SHAP values\n",
    "        shap_values = shap.Explainer(predict, X.values).shap_values(top_recommendation.name)\n",
    "        \n",
    "        # Plot SHAP values\n",
    "        shap.initjs()\n",
    "        shap.force_plot(shap.Explainer(predict, X.values).expected_value, shap_values, feature_names=X.columns)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Test the function\n",
    "explain_recommendation_with_shap('Big Hero 6', similarity_weight=1, top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab35ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
